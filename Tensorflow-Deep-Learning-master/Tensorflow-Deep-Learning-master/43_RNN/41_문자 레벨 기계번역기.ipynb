{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문자 레벨 기계번역기\n",
    "### 참조: [sequence-to-sequence 10분만에 이해하기](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)\n",
    "### 다운로드: [프랑스-영어 병렬 코퍼스](http://www.manythings.org/anki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 확인 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170651"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lines = pd.read_csv('data/fra.txt', names=['src', 'dst', 'desc'], sep='\\t')\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34551</th>\n",
       "      <td>I major in economics.</td>\n",
       "      <td>Je suis spécialisé en Sciences Économiques.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36521</th>\n",
       "      <td>That's just like you.</td>\n",
       "      <td>C'est tout toi.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45522</th>\n",
       "      <td>Don't try to be a hero.</td>\n",
       "      <td>Ne tente pas le diable !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57388</th>\n",
       "      <td>You have to do it again.</td>\n",
       "      <td>Tu dois le refaire.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14797</th>\n",
       "      <td>I went to school.</td>\n",
       "      <td>J'ai fréquenté l'école.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            src                                          dst  \\\n",
       "34551     I major in economics.  Je suis spécialisé en Sciences Économiques.   \n",
       "36521     That's just like you.                              C'est tout toi.   \n",
       "45522   Don't try to be a hero.                     Ne tente pas le diable !   \n",
       "57388  You have to do it again.                          Tu dois le refaire.   \n",
       "14797         I went to school.                      J'ai fréquenté l'école.   \n",
       "\n",
       "                                                    desc  \n",
       "34551  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "36521  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "45522  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "57388  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "14797  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 60,000개의 샘플만 가지고 기계 번역기를 구축\n",
    "lines = lines[0:60000] # 6만개만 저장\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc 컬럼 삭제\n",
    "del lines['desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35220</th>\n",
       "      <td>I'm glad you're here.</td>\n",
       "      <td>\\t Je suis contente que vous soyez là. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17116</th>\n",
       "      <td>You were seventh.</td>\n",
       "      <td>\\t Tu as été septième. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32000</th>\n",
       "      <td>Whose house is this?</td>\n",
       "      <td>\\t À qui est cette maison ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>Be careful.</td>\n",
       "      <td>\\t Soyez prudent ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46556</th>\n",
       "      <td>I feel different today.</td>\n",
       "      <td>\\t Je me sens différente, aujourd'hui. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           src                                        dst\n",
       "35220    I'm glad you're here.  \\t Je suis contente que vous soyez là. \\n\n",
       "17116        You were seventh.                  \\t Tu as été septième. \\n\n",
       "32000     Whose house is this?             \\t À qui est cette maison ? \\n\n",
       "1068               Be careful.                      \\t Soyez prudent ! \\n\n",
       "46556  I feel different today.  \\t Je me sens différente, aujourd'hui. \\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dst 열에 <sos>로 \\t, <eos>로 \\n 을 추가\n",
    "lines.dst = lines.dst.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자 집합 생성 (단어가 아님)\n",
    "src_vocab = set()\n",
    "for line in lines.src: # 1줄씩 읽음\n",
    "    for char in line: # 1개의 글자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "\n",
    "dst_vocab = set()\n",
    "for line in lines.dst:\n",
    "    for char in line:\n",
    "        dst_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 106\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "dst_vocab_size = len(dst_vocab)+1\n",
    "print(src_vocab_size, dst_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "dst_vocab = sorted(list(dst_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(dst_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, '’': 77, '€': 78}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, 'С': 100, '\\u2009': 101, '\\u200b': 102, '‘': 103, '’': 104, '\\u202f': 105}\n"
     ]
    }
   ],
   "source": [
    "# 각 글자에 인덱스 부여\n",
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "dst_to_index = dict([(word, i+1) for i, word in enumerate(dst_vocab)])\n",
    "print(src_to_index)\n",
    "print(dst_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30, 64, 10], [31, 58, 10], [31, 58, 10], [41, 70, 63, 2], [41, 70, 63, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 영어 데이터에 대한 정수 인코딩\n",
    "encoder_input = []\n",
    "for line in lines.src: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp_X = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "      temp_X.append(src_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 14, 3, 2], [1, 3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [1, 3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 프랑스어 데이터에 대한 정수 인코딩\n",
    "decoder_input = []\n",
    "for line in lines.dst:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      temp_X.append(dst_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 48, 53, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 14, 3, 2], [3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 예측값과 비교하기 위한 실제값\n",
    "# 정수 인코딩 과정에서 <sos>를 제거\n",
    "decoder_target = []\n",
    "for line in lines.dst:\n",
    "    t=0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      if t>0:\n",
    "        temp_X.append(dst_to_index[w])\n",
    "      t=t+1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 76\n"
     ]
    }
   ],
   "source": [
    "# 패딩을 위해서 영어 문장과 프랑스어 문장 각각에 대해서 가장 길이가 긴 샘플의 길이 확인\n",
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_dst_len = max([len(line) for line in lines.dst])\n",
    "print(max_src_len, max_dst_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 문장의 평균 길이 : 19.4515\n",
      "불어 문장의 평균 길이 : 28.501116666666668\n"
     ]
    }
   ],
   "source": [
    "# 평균 샘플 길이\n",
    "print('영어 문장의 평균 길이 : {}'.format(sum(map(len, lines.src))/len(lines.src)))\n",
    "print('불어 문장의 평균 길이 : {}'.format(sum(map(len, lines.dst))/len(lines.dst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩시 사용할 크기 결정\n",
    "pad_src_len = 25\n",
    "pad_dst_len = 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=pad_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=pad_dst_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=pad_dst_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원-핫 인코딩\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 교사 강요(Teacher forcing)\n",
    "- 훈련 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않고,\n",
    "- 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 하는 방법을 사용\n",
    "- RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제값을 입력으로 주는 방법을 교사 강요라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. seq2seq 기계 번역기 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, src_vocab_size), name='Encoder_Input')\n",
    "encoder_lstm = LSTM(units=256, return_state=True, name='Encoder_LSTM')\n",
    "# 인코더의 내부 상태를 디코더로 넘겨주어야 하기 때문에 return_state=True로 설정\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
    "encoder_states = [state_h, state_c]\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태.\n",
    "# 이겻이 컨텍스트 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, dst_vocab_size), name='Decoder_Input')\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True, name='Decoder_LSTM')\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Encoder_Input (InputLayer)      [(None, None, 79)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Input (InputLayer)      [(None, None, 106)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder_LSTM (LSTM)             [(None, 256), (None, 344064      Encoder_Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_LSTM (LSTM)             [(None, None, 256),  371712      Decoder_Input[0][0]              \n",
      "                                                                 Encoder_LSTM[0][1]               \n",
      "                                                                 Encoder_LSTM[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 106)    27242       Decoder_LSTM[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 743,018\n",
      "Trainable params: 743,018\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_softmax_layer = Dense(dst_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, 'image/seq2seq.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "modelpath = \"model/seq2seq-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', \n",
    "                               verbose=1, save_best_only=True)\n",
    "#early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.7812\n",
      "Epoch 00001: val_loss improved from inf to 0.68942, saving model to model/seq2seq-01-0.6894.hdf5\n",
      "48000/48000 [==============================] - 59s 1ms/sample - loss: 0.7808 - val_loss: 0.6894\n",
      "Epoch 2/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.4820\n",
      "Epoch 00002: val_loss improved from 0.68942 to 0.56079, saving model to model/seq2seq-02-0.5608.hdf5\n",
      "48000/48000 [==============================] - 61s 1ms/sample - loss: 0.4820 - val_loss: 0.5608\n",
      "Epoch 3/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.4011\n",
      "Epoch 00003: val_loss improved from 0.56079 to 0.48824, saving model to model/seq2seq-03-0.4882.hdf5\n",
      "48000/48000 [==============================] - 85s 2ms/sample - loss: 0.4011 - val_loss: 0.4882\n",
      "Epoch 4/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.3556\n",
      "Epoch 00004: val_loss improved from 0.48824 to 0.45247, saving model to model/seq2seq-04-0.4525.hdf5\n",
      "48000/48000 [==============================] - 84s 2ms/sample - loss: 0.3556 - val_loss: 0.4525\n",
      "Epoch 5/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.3260\n",
      "Epoch 00005: val_loss improved from 0.45247 to 0.42827, saving model to model/seq2seq-05-0.4283.hdf5\n",
      "48000/48000 [==============================] - 77s 2ms/sample - loss: 0.3260 - val_loss: 0.4283\n",
      "Epoch 6/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.3045\n",
      "Epoch 00006: val_loss improved from 0.42827 to 0.40985, saving model to model/seq2seq-06-0.4099.hdf5\n",
      "48000/48000 [==============================] - 89s 2ms/sample - loss: 0.3045 - val_loss: 0.4099\n",
      "Epoch 7/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.2878- ETA: 1s\n",
      "Epoch 00007: val_loss improved from 0.40985 to 0.39686, saving model to model/seq2seq-07-0.3969.hdf5\n",
      "48000/48000 [==============================] - 90s 2ms/sample - loss: 0.2878 - val_loss: 0.3969\n",
      "Epoch 8/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.2744\n",
      "Epoch 00008: val_loss improved from 0.39686 to 0.38934, saving model to model/seq2seq-08-0.3893.hdf5\n",
      "48000/48000 [==============================] - 90s 2ms/sample - loss: 0.2743 - val_loss: 0.3893\n",
      "Epoch 9/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.2633\n",
      "Epoch 00009: val_loss improved from 0.38934 to 0.38157, saving model to model/seq2seq-09-0.3816.hdf5\n",
      "48000/48000 [==============================] - 89s 2ms/sample - loss: 0.2633 - val_loss: 0.3816\n",
      "Epoch 10/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.2536\n",
      "Epoch 00010: val_loss improved from 0.38157 to 0.37691, saving model to model/seq2seq-10-0.3769.hdf5\n",
      "48000/48000 [==============================] - 92s 2ms/sample - loss: 0.2536 - val_loss: 0.3769\n",
      "Epoch 11/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.2453\n",
      "Epoch 00011: val_loss improved from 0.37691 to 0.37307, saving model to model/seq2seq-11-0.3731.hdf5\n",
      "48000/48000 [==============================] - 89s 2ms/sample - loss: 0.2452 - val_loss: 0.3731\n",
      "Epoch 12/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.2379\n",
      "Epoch 00012: val_loss improved from 0.37307 to 0.36944, saving model to model/seq2seq-12-0.3694.hdf5\n",
      "48000/48000 [==============================] - 95s 2ms/sample - loss: 0.2379 - val_loss: 0.3694\n",
      "Epoch 13/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.2312\n",
      "Epoch 00013: val_loss improved from 0.36944 to 0.36762, saving model to model/seq2seq-13-0.3676.hdf5\n",
      "48000/48000 [==============================] - 84s 2ms/sample - loss: 0.2312 - val_loss: 0.3676\n",
      "Epoch 14/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.2251\n",
      "Epoch 00014: val_loss improved from 0.36762 to 0.36714, saving model to model/seq2seq-14-0.3671.hdf5\n",
      "48000/48000 [==============================] - 99s 2ms/sample - loss: 0.2251 - val_loss: 0.3671\n",
      "Epoch 15/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.2196\n",
      "Epoch 00015: val_loss improved from 0.36714 to 0.36530, saving model to model/seq2seq-15-0.3653.hdf5\n",
      "48000/48000 [==============================] - 89s 2ms/sample - loss: 0.2196 - val_loss: 0.3653\n",
      "Epoch 16/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.2145\n",
      "Epoch 00016: val_loss improved from 0.36530 to 0.36498, saving model to model/seq2seq-16-0.3650.hdf5\n",
      "48000/48000 [==============================] - 93s 2ms/sample - loss: 0.2144 - val_loss: 0.3650\n",
      "Epoch 17/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.2097\n",
      "Epoch 00017: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 92s 2ms/sample - loss: 0.2097 - val_loss: 0.3660\n",
      "Epoch 18/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.2051\n",
      "Epoch 00018: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 85s 2ms/sample - loss: 0.2051 - val_loss: 0.3677\n",
      "Epoch 19/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.2010\n",
      "Epoch 00019: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 102s 2ms/sample - loss: 0.2010 - val_loss: 0.3665\n",
      "Epoch 20/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1970\n",
      "Epoch 00020: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 95s 2ms/sample - loss: 0.1970 - val_loss: 0.3684\n",
      "Epoch 21/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1933\n",
      "Epoch 00021: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 94s 2ms/sample - loss: 0.1933 - val_loss: 0.3695\n",
      "Epoch 22/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1897\n",
      "Epoch 00022: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 89s 2ms/sample - loss: 0.1897 - val_loss: 0.3714\n",
      "Epoch 23/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1863\n",
      "Epoch 00023: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 84s 2ms/sample - loss: 0.1863 - val_loss: 0.3742\n",
      "Epoch 24/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1830\n",
      "Epoch 00024: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 97s 2ms/sample - loss: 0.1830 - val_loss: 0.3743\n",
      "Epoch 25/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1800\n",
      "Epoch 00025: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 86s 2ms/sample - loss: 0.1800 - val_loss: 0.3783\n",
      "Epoch 26/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1770\n",
      "Epoch 00026: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 100s 2ms/sample - loss: 0.1770 - val_loss: 0.3778\n",
      "Epoch 27/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1743\n",
      "Epoch 00027: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 99s 2ms/sample - loss: 0.1743 - val_loss: 0.3814\n",
      "Epoch 28/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1716\n",
      "Epoch 00028: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 95s 2ms/sample - loss: 0.1716 - val_loss: 0.3846\n",
      "Epoch 29/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1691\n",
      "Epoch 00029: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 101s 2ms/sample - loss: 0.1691 - val_loss: 0.3853\n",
      "Epoch 30/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1667\n",
      "Epoch 00030: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 103s 2ms/sample - loss: 0.1667 - val_loss: 0.3884\n",
      "Epoch 31/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1644\n",
      "Epoch 00031: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 105s 2ms/sample - loss: 0.1644 - val_loss: 0.3913\n",
      "Epoch 32/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1620\n",
      "Epoch 00032: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 109s 2ms/sample - loss: 0.1620 - val_loss: 0.3945\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1599\n",
      "Epoch 00033: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 108s 2ms/sample - loss: 0.1599 - val_loss: 0.3966\n",
      "Epoch 34/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1578\n",
      "Epoch 00034: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 97s 2ms/sample - loss: 0.1578 - val_loss: 0.4000\n",
      "Epoch 35/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1559\n",
      "Epoch 00035: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 110s 2ms/sample - loss: 0.1558 - val_loss: 0.4007\n",
      "Epoch 36/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1540\n",
      "Epoch 00036: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 106s 2ms/sample - loss: 0.1540 - val_loss: 0.4045\n",
      "Epoch 37/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1521\n",
      "Epoch 00037: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 109s 2ms/sample - loss: 0.1521 - val_loss: 0.4064\n",
      "Epoch 38/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1502\n",
      "Epoch 00038: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 114s 2ms/sample - loss: 0.1502 - val_loss: 0.4077\n",
      "Epoch 39/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1486\n",
      "Epoch 00039: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 114s 2ms/sample - loss: 0.1486 - val_loss: 0.4119\n",
      "Epoch 40/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1468\n",
      "Epoch 00040: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 113s 2ms/sample - loss: 0.1469 - val_loss: 0.4138\n",
      "Epoch 41/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1454\n",
      "Epoch 00041: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 114s 2ms/sample - loss: 0.1454 - val_loss: 0.4178\n",
      "Epoch 42/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1438\n",
      "Epoch 00042: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 114s 2ms/sample - loss: 0.1438 - val_loss: 0.4191\n",
      "Epoch 43/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1422\n",
      "Epoch 00043: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 114s 2ms/sample - loss: 0.1422 - val_loss: 0.4232\n",
      "Epoch 44/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1408\n",
      "Epoch 00044: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 114s 2ms/sample - loss: 0.1408 - val_loss: 0.4240\n",
      "Epoch 45/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1393\n",
      "Epoch 00045: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 114s 2ms/sample - loss: 0.1393 - val_loss: 0.4253\n",
      "Epoch 46/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1381\n",
      "Epoch 00046: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 114s 2ms/sample - loss: 0.1382 - val_loss: 0.4273\n",
      "Epoch 47/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1368\n",
      "Epoch 00047: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 115s 2ms/sample - loss: 0.1368 - val_loss: 0.4312\n",
      "Epoch 48/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1356\n",
      "Epoch 00048: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 114s 2ms/sample - loss: 0.1356 - val_loss: 0.4328\n",
      "Epoch 49/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1342\n",
      "Epoch 00049: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 114s 2ms/sample - loss: 0.1343 - val_loss: 0.4362\n",
      "Epoch 50/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1331\n",
      "Epoch 00050: val_loss did not improve from 0.36498\n",
      "48000/48000 [==============================] - 114s 2ms/sample - loss: 0.1331 - val_loss: 0.4384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f014f4ae48>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target,\n",
    "          batch_size=64, epochs=50, validation_split=0.2,\n",
    "          callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. seq2seq 기계 번역기 동작시키기\n",
    "- 학습한 최종 모델로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태를 이전 상태로 사용\n",
    "decoder_states = [state_h, state_c]\n",
    "# 이번에는 훈련 과정에서와 달리 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict(\n",
    "    (i, char) for char, i in src_to_index.items())\n",
    "index_to_dst = dict(\n",
    "    (i, char) for char, i in dst_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, dst_vocab_size))\n",
    "    target_seq[0, 0, dst_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition: #stop_condition이 True가 될 때까지 루프 반복\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_dst[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <sos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_dst_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트 합니다.\n",
    "        target_seq = np.zeros((1, 1, dst_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장:  Cours ! \n",
      "번역기가 번역한 문장:  Courez ! \n",
      "-----------------------------------\n",
      "입력 문장: I lost.\n",
      "정답 문장:  J'ai perdu. \n",
      "번역기가 번역한 문장:  J'ai perdu. \n",
      "-----------------------------------\n",
      "입력 문장: Come in.\n",
      "정답 문장:  Entre ! \n",
      "번역기가 번역한 문장:  Entrez ! \n",
      "-----------------------------------\n",
      "입력 문장: I got it.\n",
      "정답 문장:  J'ai capté. \n",
      "번역기가 번역한 문장:  Je l'ai confectionné. \n",
      "-----------------------------------\n",
      "입력 문장: Who cares?\n",
      "정답 문장:  Qui s'en préoccupe ? \n",
      "번역기가 번역한 문장:  Qui s'en soucie ? \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.dst[seq_index][1:len(lines.dst[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. seq2seq 기계 번역기 동작시키기\n",
    "- Best 모델로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model 선택\n",
    "from tensorflow.keras.models import load_model\n",
    "del model\n",
    "model = load_model('model/seq2seq-16-0.3650.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태를 이전 상태로 사용\n",
    "decoder_states = [state_h, state_c]\n",
    "# 이번에는 훈련 과정에서와 달리 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장:  Cours ! \n",
      "번역기가 번역한 문장:  Courez ! \n",
      "-----------------------------------\n",
      "입력 문장: I lost.\n",
      "정답 문장:  J'ai perdu. \n",
      "번역기가 번역한 문장:  J'ai perdu. \n",
      "-----------------------------------\n",
      "입력 문장: Come in.\n",
      "정답 문장:  Entre ! \n",
      "번역기가 번역한 문장:  Entrez ! \n",
      "-----------------------------------\n",
      "입력 문장: I got it.\n",
      "정답 문장:  J'ai capté. \n",
      "번역기가 번역한 문장:  Je l'ai confectionné. \n",
      "-----------------------------------\n",
      "입력 문장: Who cares?\n",
      "정답 문장:  Qui s'en préoccupe ? \n",
      "번역기가 번역한 문장:  Qui s'en soucie ? \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.dst[seq_index][1:len(lines.dst[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
